{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19efae20",
   "metadata": {
    "id": "19efae20"
   },
   "source": [
    "**1. Смотрим на Hadoop Distributed File System**\n",
    "\n",
    "В рамках этой части вам нужно будет обращаться к HDFS с помощью CLI, разместить файлы для следующих заданий в распределеннй файловой системе и выполнить несколько преобразований над ними.\n",
    "\n",
    "Для работы файлы можно скачать по следующим ссылкам:\n",
    "- Логи посещения сайтов юзерами за некоторый промежуток времени [ссылка](https://drive.google.com/file/d/1WXyq5WVSWwJYXPuH4kyAJ5mrR3XgfO_H/view?usp=sharing)\n",
    "\n",
    "Разместите их в нашем внутреннем файловом хранилище с помощью HDFS CLI, для дальнейшего удобства под каждый файл стоит создать каталог с простым и понятным именем, разместить сами файлы в разных каталогах.\n",
    "\n",
    "Набор комманд, которые вам могут в этом помочь, доступны [здесь](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/FileSystemShell.html)\n",
    "\n",
    "В ячейках ниже должен быть полный набор комманд ваших обращей к консоли"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b4c0f990",
   "metadata": {
    "id": "b4c0f990",
    "outputId": "4b0cd4da-bd10-4619-ac02-cda922c3fd64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 items\r\n",
      "drwxr-xr-x   - hdfs   hadoop          0 2025-04-15 20:51 /bible_text\r\n",
      "drwx------   - mapred hadoop          0 2025-04-15 12:47 /hadoop\r\n",
      "drwxrwxrwt   - hdfs   hadoop          0 2025-04-15 12:47 /tmp\r\n",
      "drwxrwxrwt   - hdfs   hadoop          0 2025-04-15 20:54 /user\r\n",
      "drwxr-xr-x   - hdfs   hadoop          0 2025-04-15 20:51 /user_logs\r\n",
      "drwxrwxrwt   - hdfs   hadoop          0 2025-04-15 12:47 /var\r\n"
     ]
    }
   ],
   "source": [
    "## вы можете обращаться к консоли из ноутбука таким способом\n",
    "!hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "41a5a267",
   "metadata": {
    "id": "41a5a267",
    "outputId": "33b4bc5e-9527-4241-a541-83ae98ce3891"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 items\n",
      "drwxr-xr-x   - hdfs   hadoop          0 2025-04-15 20:51 /bible_text\n",
      "drwx------   - mapred hadoop          0 2025-04-15 12:47 /hadoop\n",
      "drwxrwxrwt   - hdfs   hadoop          0 2025-04-15 12:47 /tmp\n",
      "drwxrwxrwt   - hdfs   hadoop          0 2025-04-15 20:54 /user\n",
      "drwxr-xr-x   - hdfs   hadoop          0 2025-04-15 20:51 /user_logs\n",
      "drwxrwxrwt   - hdfs   hadoop          0 2025-04-15 12:47 /var\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "## или же использовать для этого меджик строчку в ячейке %%bash, как вам будет удобнее\n",
    "\n",
    "hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "61786cf4",
   "metadata": {
    "id": "61786cf4",
    "outputId": "51760388-8777-4b84-c564-d28e769ba12f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: `/input': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls -R -h /input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a0c6e16b",
   "metadata": {
    "id": "a0c6e16b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drwxr-xr-x   - ubuntu hadoop          0 2025-04-15 20:54 /user/ubuntu/bible_text\n",
      "-rw-r--r--   1 ubuntu hadoop    4047392 2025-04-15 20:54 /user/ubuntu/bible_text/bible.txt\n",
      "drwxr-xr-x   - ubuntu hadoop          0 2025-04-15 20:54 /user/ubuntu/user_logs\n",
      "-rw-r--r--   1 ubuntu hadoop   36443383 2025-04-15 20:54 /user/ubuntu/user_logs/user_logs.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "put: `/user/ubuntu/user_logs/user_logs.csv': File exists\n",
      "put: `/user/ubuntu/bible_text/bible.txt': File exists\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# 1. Создаем директории в домашней папке пользователя\n",
    "hdfs dfs -mkdir -p /user/ubuntu/user_logs\n",
    "hdfs dfs -mkdir -p /user/ubuntu/bible_text\n",
    "\n",
    "# 2. Загружаем файлы\n",
    "hdfs dfs -put user_logs.csv /user/ubuntu/user_logs/\n",
    "hdfs dfs -put bible.txt /user/ubuntu/bible_text/\n",
    "\n",
    "# 3. Проверяем\n",
    "hdfs dfs -ls -R /user/ubuntu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1f83ac",
   "metadata": {
    "id": "1e1f83ac"
   },
   "source": [
    "**2. Решаем задачи MapReduce**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990332ef",
   "metadata": {
    "id": "990332ef"
   },
   "source": [
    "**2.1 Подсчет слов в тексте**\n",
    "\n",
    "В рамках данного задания вам нужно подсчитать кол-во слов в тексте Библии (файл приложен к ДЗ в чате тг), то есть необходимо реализовать базовый функционал утилиты word count.\n",
    "\n",
    "**Важно** - подсчитывайте число только тех слов, длина которых больше 4 символов. Проводить процесс удаления знаков препинания и прочих символов **не нужно**\n",
    "\n",
    "Ниже вам представлены ячейки, в которых вы должны описать структуру маппера/редьсюера и ниже вызвать их в bash-скрипте запуска MR-таски"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "295c61fd",
   "metadata": {
    "id": "295c61fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/env python3\n",
    "import sys\n",
    "import re\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip().split()\n",
    "    for word in words:\n",
    "        if len(word) > 4:\n",
    "            print(f\"{word.lower()}\\t1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1bef060",
   "metadata": {
    "id": "c1bef060"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/env python3\n",
    "import sys\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    word, count = line.split('\\t', 1)\n",
    "    \n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        continue\n",
    "    \n",
    "    if current_word == word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            print(f\"{current_word}\\t{current_count}\")\n",
    "        current_word = word\n",
    "        current_count = count\n",
    "\n",
    "if current_word == word:\n",
    "    print(f\"{current_word}\\t{current_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b35cd9",
   "metadata": {
    "id": "97b35cd9"
   },
   "source": [
    "В качестве проверки ваших python-скриптов до запуска MR таски можно произвести их запуск через консольные команды\n",
    "\n",
    "Тогда наша задача не будет выполняться через датаноды, а только на локальной машине, но в случае ошибок в скриптах вы увидите их и сможете исправить"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a5975e36",
   "metadata": {
    "id": "a5975e36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "goodbye\t1\n",
      "hadoop\t2\n",
      "hello\t2\n",
      "world\t1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# 1. Создаем тестовый файл (если нет file.txt)\n",
    "echo -e \"Hello World\\nHello Hadoop\\nGoodbye Hadoop\" > file.txt\n",
    "\n",
    "# 2. Запускаем цепочку MapReduce\n",
    "cat file.txt | python3 mapper.py | sort -k1,1 | python3 reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8279f6c1",
   "metadata": {
    "id": "8279f6c1"
   },
   "source": [
    "Как только в данной проверке вы получите успешный и корректный результат, можете запустить Map Reduce в ячейке ниже"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1ecb1749",
   "metadata": {
    "id": "1ecb1749"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-3.2.2.jar] /tmp/streamjob6447965003391667163.jar tmpDir=null\n",
      "prayer\t107\n",
      "prayed\t59\n",
      "lords\t42\n",
      "ungodly\t27\n",
      "prayers\t24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 21:28:58,176 INFO client.RMProxy: Connecting to ResourceManager at rc1b-dataproc-m-i45rlr66g11r9d3u.mdb.yandexcloud.net/10.129.0.25:8032\n",
      "2025-04-15 21:28:58,405 INFO client.AHSProxy: Connecting to Application History server at rc1b-dataproc-m-i45rlr66g11r9d3u.mdb.yandexcloud.net/10.129.0.25:10200\n",
      "2025-04-15 21:28:58,438 INFO client.RMProxy: Connecting to ResourceManager at rc1b-dataproc-m-i45rlr66g11r9d3u.mdb.yandexcloud.net/10.129.0.25:8032\n",
      "2025-04-15 21:28:58,439 INFO client.AHSProxy: Connecting to Application History server at rc1b-dataproc-m-i45rlr66g11r9d3u.mdb.yandexcloud.net/10.129.0.25:10200\n",
      "2025-04-15 21:28:58,658 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/ubuntu/.staging/job_1744740277109_0011\n",
      "2025-04-15 21:28:59,081 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2025-04-15 21:28:59,381 INFO mapreduce.JobSubmitter: number of splits:30\n",
      "2025-04-15 21:29:00,009 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1744740277109_0011\n",
      "2025-04-15 21:29:00,013 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2025-04-15 21:29:00,202 INFO conf.Configuration: resource-types.xml not found\n",
      "2025-04-15 21:29:00,203 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2025-04-15 21:29:00,290 INFO impl.YarnClientImpl: Submitted application application_1744740277109_0011\n",
      "2025-04-15 21:29:00,328 INFO mapreduce.Job: The url to track the job: http://rc1b-dataproc-m-i45rlr66g11r9d3u.mdb.yandexcloud.net:8088/proxy/application_1744740277109_0011/\n",
      "2025-04-15 21:29:00,330 INFO mapreduce.Job: Running job: job_1744740277109_0011\n",
      "2025-04-15 21:29:07,533 INFO mapreduce.Job: Job job_1744740277109_0011 running in uber mode : false\n",
      "2025-04-15 21:29:07,536 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2025-04-15 21:29:13,773 INFO mapreduce.Job:  map 3% reduce 0%\n",
      "2025-04-15 21:29:14,788 INFO mapreduce.Job:  map 7% reduce 0%\n",
      "2025-04-15 21:29:16,810 INFO mapreduce.Job:  map 10% reduce 0%\n",
      "2025-04-15 21:29:18,828 INFO mapreduce.Job:  map 13% reduce 0%\n",
      "2025-04-15 21:29:20,845 INFO mapreduce.Job:  map 23% reduce 0%\n",
      "2025-04-15 21:29:22,866 INFO mapreduce.Job:  map 27% reduce 0%\n",
      "2025-04-15 21:29:23,877 INFO mapreduce.Job:  map 30% reduce 0%\n",
      "2025-04-15 21:29:26,901 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "2025-04-15 21:29:29,928 INFO mapreduce.Job:  map 40% reduce 0%\n",
      "2025-04-15 21:29:30,936 INFO mapreduce.Job:  map 47% reduce 0%\n",
      "2025-04-15 21:29:32,959 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "2025-04-15 21:29:34,974 INFO mapreduce.Job:  map 53% reduce 1%\n",
      "2025-04-15 21:29:40,017 INFO mapreduce.Job:  map 63% reduce 3%\n",
      "2025-04-15 21:29:41,025 INFO mapreduce.Job:  map 67% reduce 3%\n",
      "2025-04-15 21:29:46,067 INFO mapreduce.Job:  map 67% reduce 5%\n",
      "2025-04-15 21:29:47,074 INFO mapreduce.Job:  map 70% reduce 6%\n",
      "2025-04-15 21:29:50,094 INFO mapreduce.Job:  map 80% reduce 8%\n",
      "2025-04-15 21:29:53,114 INFO mapreduce.Job:  map 80% reduce 9%\n",
      "2025-04-15 21:29:56,140 INFO mapreduce.Job:  map 83% reduce 9%\n",
      "2025-04-15 21:29:57,146 INFO mapreduce.Job:  map 87% reduce 9%\n",
      "2025-04-15 21:29:59,163 INFO mapreduce.Job:  map 90% reduce 9%\n",
      "2025-04-15 21:30:00,174 INFO mapreduce.Job:  map 97% reduce 9%\n",
      "2025-04-15 21:30:02,189 INFO mapreduce.Job:  map 97% reduce 10%\n",
      "2025-04-15 21:30:05,214 INFO mapreduce.Job:  map 97% reduce 11%\n",
      "2025-04-15 21:30:06,223 INFO mapreduce.Job:  map 100% reduce 11%\n",
      "2025-04-15 21:30:07,231 INFO mapreduce.Job:  map 100% reduce 16%\n",
      "2025-04-15 21:30:08,239 INFO mapreduce.Job:  map 100% reduce 42%\n",
      "2025-04-15 21:30:09,247 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "2025-04-15 21:30:10,252 INFO mapreduce.Job:  map 100% reduce 58%\n",
      "2025-04-15 21:30:11,260 INFO mapreduce.Job:  map 100% reduce 67%\n",
      "2025-04-15 21:30:12,268 INFO mapreduce.Job:  map 100% reduce 75%\n",
      "2025-04-15 21:30:16,298 INFO mapreduce.Job:  map 100% reduce 83%\n",
      "2025-04-15 21:30:17,306 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2025-04-15 21:30:18,327 INFO mapreduce.Job: Job job_1744740277109_0011 completed successfully\n",
      "2025-04-15 21:30:18,448 INFO mapreduce.Job: Counters: 57\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=87959\n",
      "\t\tFILE: Number of bytes written=10647510\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=7737421\n",
      "\t\tHDFS: Number of bytes written=119670\n",
      "\t\tHDFS: Number of read operations=150\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=36\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=30\n",
      "\t\tLaunched reduce tasks=12\n",
      "\t\tData-local map tasks=16\n",
      "\t\tRack-local map tasks=14\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=188543\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=213924\n",
      "\t\tTotal time spent by all map tasks (ms)=188543\n",
      "\t\tTotal time spent by all reduce tasks (ms)=213924\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=188543\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=213924\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=193068032\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=219058176\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=30383\n",
      "\t\tMap output records=236831\n",
      "\t\tMap output bytes=2241464\n",
      "\t\tMap output materialized bytes=360465\n",
      "\t\tInput split bytes=4320\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=11040\n",
      "\t\tReduce shuffle bytes=360465\n",
      "\t\tReduce input records=236831\n",
      "\t\tReduce output records=11040\n",
      "\t\tSpilled Records=473662\n",
      "\t\tShuffled Maps =360\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=360\n",
      "\t\tGC time elapsed (ms)=5814\n",
      "\t\tCPU time spent (ms)=43200\n",
      "\t\tPhysical memory (bytes) snapshot=11065057280\n",
      "\t\tVirtual memory (bytes) snapshot=107144765440\n",
      "\t\tTotal committed heap usage (bytes)=9142534144\n",
      "\t\tPeak Map Physical memory (bytes)=293744640\n",
      "\t\tPeak Map Virtual memory (bytes)=2551975936\n",
      "\t\tPeak Reduce Physical memory (bytes)=206172160\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2563215360\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=7733101\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=119670\n",
      "2025-04-15 21:30:18,448 INFO streaming.StreamJob: Output directory: /user/ubuntu/word_count_task\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Clean previous output if exists (using user directory instead of root)\n",
    "hdfs dfs -rm -r /user/ubuntu/word_count_task 2> /dev/null || true\n",
    "\n",
    "# Run the MapReduce job with correct paths\n",
    "hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "    -D mapreduce.job.name=\"word-count\" \\\n",
    "    -files ./mapper.py,./reducer.py \\\n",
    "    -mapper \"python3 mapper.py\" \\\n",
    "    -reducer \"python3 reducer.py\" \\\n",
    "    -input /user/ubuntu/bible_text/bible.txt \\\n",
    "    -output /user/ubuntu/word_count_task\n",
    "\n",
    "# Check the results\n",
    "hdfs dfs -cat /user/ubuntu/word_count_task/* | grep -E 'lord|god|pray' | sort -t$'\\t' -k2 -nr | head -n 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9c0ff5",
   "metadata": {
    "id": "fe9c0ff5"
   },
   "source": [
    "Мониторить процесс работы таски можно на nodemanager по порту 8088 (уже прокинут в конфиге), там будет UI, в котором будет видно вашу запущенную задачу и её статус."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f62cdb6",
   "metadata": {
    "id": "4f62cdb6"
   },
   "source": [
    "Результат работы скрипта должен выглядеть следующим образом (вывод тестовый):\n",
    "\n",
    "```bash\n",
    "word count\n",
    "abtr 6852\n",
    "btoad 4237\n",
    "stress 1932\n",
    "zen 1885\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "868c1f67",
   "metadata": {
    "id": "868c1f67"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "## запустите эту команду, чтобы вывести счетчик определенных слов, которые мы указали на grep\n",
    "## Это нам будет необходимо для визуального анализа результата работы вашего скрипта\n",
    "## в sort можете указать тот разделитель колонок, с которым у вас результат выплевывает редьюсер\n",
    "\n",
    "\n",
    "## Сделал в ячейке выше\n",
    "hdfs dfs -cat /user/ubuntu/word_count_task/* | grep  -E 'lord\\.|god\\.|pray\\.' | sort -t$'\\t' -k2.2nr  | head -n 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1511d296",
   "metadata": {
    "id": "1511d296"
   },
   "source": [
    "**2.2 Решаем задачу поиска самых посещаемых сайтов**\n",
    "\n",
    "В данном задании нужно поработать с логом данных о посещении юзерами различных сайтов.\n",
    "Формат данных: `url;временная метка`. Вам нужно вывести топ 5 сайтов по посещаемости в каждую из дат, которая представлена в наших данных.\n",
    "\n",
    "Результат работы скрипта должен выглядеть следующим образом:\n",
    "\n",
    "```bash\n",
    "date        site                            count\n",
    "2024-05-25  https://gonzales-bautista.com/  987\n",
    "2024-05-25  https://smith.com/              654\n",
    "2024-05-25  https://www.smith.com/          321\n",
    "```\n",
    "\n",
    "**Рекомендации**\n",
    "\n",
    "1. Вам могу пригодиться дополнительные параметры mr таски, отвечающие за настройку шаффла, и правил сортировки ключей внутри него. Почитать о примерах их использования можно [здесь](https://hadoop.apache.org/docs/current/hadoop-streaming/HadoopStreaming.html#More_Usage_Examples).\n",
    "\n",
    "2. Не рекомендуем использовать `\\t` в качестве символа разделителя для сложного ключа (потому что по дефолту таб используется для разделения колонок данных, и ключом в таком случае будет только первая колонка до таба). Если вы будете собирать сложный ключ для нужной вам сортировки данных, лучше всего будет использовать другие симловы, к примеру `+, =`.\n",
    "\n",
    "3. Возможно, у вас не получится решить данную задачу за одну mr таску, тогда вы просто описываете в решении скрипты ваших мапперов, редьюсеров под каждую из mr тасок, которые вам нужно запустить для получения нужного результата.\n",
    "\n",
    "**Важно** помнить, что любой маппер и редьюсер должен работать за O(1) памяти, и если вы будете создавать какой-то список, куда будете складывать какие-то данные, то он не должен быть размера O(n). Если такой момент в вашем решении будет, пожалуйста, поясните его текстово, что с вашими переменными все ок, и у них нет размера O(n)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0cfe909b",
   "metadata": {
    "id": "0cfe909b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper1.py\n",
    "#!/usr/bin/env python3\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        url, timestamp = line.split(';')\n",
    "        date = timestamp.split('T')[0]\n",
    "        print(f\"{date}+{url}\\t1\")\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7be2523a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing reducer1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer1.py\n",
    "#!/usr/bin/env python3\n",
    "import sys\n",
    "\n",
    "current_key = None\n",
    "current_count = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    key, count = line.split('\\t', 1)\n",
    "    \n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        continue\n",
    "    \n",
    "    if current_key == key:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_key:\n",
    "            print(f\"{current_key}\\t{current_count}\")\n",
    "        current_key = key\n",
    "        current_count = count\n",
    "\n",
    "if current_key == key:\n",
    "    print(f\"{current_key}\\t{current_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "075607c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mapper2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper2.py\n",
    "#!/usr/bin/env python3\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        key, count = line.split('\\t')\n",
    "        date, url = key.split('+')\n",
    "        print(f\"{date}\\t{count}+{url}\")\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "df7df37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing reducer2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer2.py\n",
    "#!/usr/bin/env python3\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "current_date = None\n",
    "url_counts = []\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    date, count_url = line.split('\\t', 1)\n",
    "    count, url = count_url.split('+', 1)\n",
    "    \n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        continue\n",
    "    \n",
    "    if current_date == date:\n",
    "        url_counts.append((count, url))\n",
    "    else:\n",
    "        if current_date:\n",
    "            url_counts.sort(reverse=True)\n",
    "            for cnt, u in url_counts[:5]:\n",
    "                print(f\"{current_date}\\t{u}\\t{cnt}\")\n",
    "        current_date = date\n",
    "        url_counts = [(count, url)]\n",
    "\n",
    "if current_date == date:\n",
    "    url_counts.sort(reverse=True)\n",
    "    for cnt, u in url_counts[:5]:\n",
    "        print(f\"{current_date}\\t{u}\\t{cnt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2ffeefcb",
   "metadata": {
    "id": "2ffeefcb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "-rw-r--r--   1 ubuntu hadoop   36443383 2025-04-15 20:54 /user/ubuntu/user_logs/user_logs.csv\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-3.2.2.jar] /tmp/streamjob1087441329380737641.jar tmpDir=null\n",
      "Found 5 items\n",
      "-rw-r--r--   1 ubuntu hadoop          0 2025-04-15 21:49 /user/ubuntu/site_count_temp/_SUCCESS\n",
      "-rw-r--r--   1 ubuntu hadoop    9257049 2025-04-15 21:49 /user/ubuntu/site_count_temp/part-00000\n",
      "-rw-r--r--   1 ubuntu hadoop    9298852 2025-04-15 21:49 /user/ubuntu/site_count_temp/part-00001\n",
      "-rw-r--r--   1 ubuntu hadoop    9277325 2025-04-15 21:49 /user/ubuntu/site_count_temp/part-00002\n",
      "-rw-r--r--   1 ubuntu hadoop    9306070 2025-04-15 21:49 /user/ubuntu/site_count_temp/part-00003\n",
      "2024-05-26 00:35:09.853479+http://www.knight.biz/\t1\n",
      "2024-05-26 00:35:16.853479+https://www.schmitt.biz/\t1\n",
      "2024-05-26 00:35:19.853479+http://nelson.com/\t1\n",
      "2024-05-26 00:35:21.853479+http://stewart.com/\t1\n",
      "2024-05-26 00:35:25.853479+https://lester.net/\t1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 21:48:46,161 INFO client.RMProxy: Connecting to ResourceManager at rc1b-dataproc-m-i45rlr66g11r9d3u.mdb.yandexcloud.net/10.129.0.25:8032\n",
      "2025-04-15 21:48:46,384 INFO client.AHSProxy: Connecting to Application History server at rc1b-dataproc-m-i45rlr66g11r9d3u.mdb.yandexcloud.net/10.129.0.25:10200\n",
      "2025-04-15 21:48:46,424 INFO client.RMProxy: Connecting to ResourceManager at rc1b-dataproc-m-i45rlr66g11r9d3u.mdb.yandexcloud.net/10.129.0.25:8032\n",
      "2025-04-15 21:48:46,425 INFO client.AHSProxy: Connecting to Application History server at rc1b-dataproc-m-i45rlr66g11r9d3u.mdb.yandexcloud.net/10.129.0.25:10200\n",
      "2025-04-15 21:48:46,621 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/ubuntu/.staging/job_1744740277109_0012\n",
      "2025-04-15 21:48:46,982 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2025-04-15 21:48:47,076 INFO mapreduce.JobSubmitter: number of splits:30\n",
      "2025-04-15 21:48:47,250 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1744740277109_0012\n",
      "2025-04-15 21:48:47,252 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2025-04-15 21:48:47,459 INFO conf.Configuration: resource-types.xml not found\n",
      "2025-04-15 21:48:47,460 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2025-04-15 21:48:47,558 INFO impl.YarnClientImpl: Submitted application application_1744740277109_0012\n",
      "2025-04-15 21:48:47,598 INFO mapreduce.Job: The url to track the job: http://rc1b-dataproc-m-i45rlr66g11r9d3u.mdb.yandexcloud.net:8088/proxy/application_1744740277109_0012/\n",
      "2025-04-15 21:48:47,599 INFO mapreduce.Job: Running job: job_1744740277109_0012\n",
      "2025-04-15 21:48:54,794 INFO mapreduce.Job: Job job_1744740277109_0012 running in uber mode : false\n",
      "2025-04-15 21:48:54,797 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2025-04-15 21:49:00,914 INFO mapreduce.Job:  map 3% reduce 0%\n",
      "2025-04-15 21:49:02,943 INFO mapreduce.Job:  map 7% reduce 0%\n",
      "2025-04-15 21:49:03,955 INFO mapreduce.Job:  map 13% reduce 0%\n",
      "2025-04-15 21:49:04,965 INFO mapreduce.Job:  map 17% reduce 0%\n",
      "2025-04-15 21:49:09,006 INFO mapreduce.Job:  map 20% reduce 0%\n",
      "2025-04-15 21:49:10,016 INFO mapreduce.Job:  map 23% reduce 0%\n",
      "2025-04-15 21:49:11,026 INFO mapreduce.Job:  map 30% reduce 0%\n",
      "2025-04-15 21:49:12,037 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "2025-04-15 21:49:16,074 INFO mapreduce.Job:  map 37% reduce 0%\n",
      "2025-04-15 21:49:17,084 INFO mapreduce.Job:  map 40% reduce 0%\n",
      "2025-04-15 21:49:18,093 INFO mapreduce.Job:  map 43% reduce 0%\n",
      "2025-04-15 21:49:19,101 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "2025-04-15 21:49:20,110 INFO mapreduce.Job:  map 50% reduce 4%\n",
      "2025-04-15 21:49:21,121 INFO mapreduce.Job:  map 53% reduce 4%\n",
      "2025-04-15 21:49:24,148 INFO mapreduce.Job:  map 60% reduce 4%\n",
      "2025-04-15 21:49:26,165 INFO mapreduce.Job:  map 60% reduce 5%\n",
      "2025-04-15 21:49:27,176 INFO mapreduce.Job:  map 63% reduce 10%\n",
      "2025-04-15 21:49:29,195 INFO mapreduce.Job:  map 67% reduce 10%\n",
      "2025-04-15 21:49:30,203 INFO mapreduce.Job:  map 70% reduce 10%\n",
      "2025-04-15 21:49:32,222 INFO mapreduce.Job:  map 70% reduce 11%\n",
      "2025-04-15 21:49:33,231 INFO mapreduce.Job:  map 70% reduce 17%\n",
      "2025-04-15 21:49:34,243 INFO mapreduce.Job:  map 73% reduce 17%\n",
      "2025-04-15 21:49:35,252 INFO mapreduce.Job:  map 80% reduce 17%\n",
      "2025-04-15 21:49:36,261 INFO mapreduce.Job:  map 80% reduce 24%\n",
      "2025-04-15 21:49:39,288 INFO mapreduce.Job:  map 80% reduce 26%\n",
      "2025-04-15 21:49:40,298 INFO mapreduce.Job:  map 83% reduce 26%\n",
      "2025-04-15 21:49:41,306 INFO mapreduce.Job:  map 90% reduce 26%\n",
      "2025-04-15 21:49:42,315 INFO mapreduce.Job:  map 93% reduce 27%\n",
      "2025-04-15 21:49:44,333 INFO mapreduce.Job:  map 93% reduce 28%\n",
      "2025-04-15 21:49:45,343 INFO mapreduce.Job:  map 100% reduce 30%\n",
      "2025-04-15 21:49:48,367 INFO mapreduce.Job:  map 100% reduce 42%\n",
      "2025-04-15 21:49:49,374 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2025-04-15 21:49:50,395 INFO mapreduce.Job: Job job_1744740277109_0012 completed successfully\n",
      "2025-04-15 21:49:50,505 INFO mapreduce.Job: Counters: 55\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=6857047\n",
      "\t\tFILE: Number of bytes written=22898613\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=39222512\n",
      "\t\tHDFS: Number of bytes written=37139296\n",
      "\t\tHDFS: Number of read operations=110\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=12\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=30\n",
      "\t\tLaunched reduce tasks=4\n",
      "\t\tData-local map tasks=15\n",
      "\t\tRack-local map tasks=15\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=138073\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=135988\n",
      "\t\tTotal time spent by all map tasks (ms)=138073\n",
      "\t\tTotal time spent by all reduce tasks (ms)=135988\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=138073\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=135988\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=141386752\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=139251712\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=700000\n",
      "\t\tMap output records=700000\n",
      "\t\tMap output bytes=37143383\n",
      "\t\tMap output materialized bytes=7788690\n",
      "\t\tInput split bytes=4410\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=699921\n",
      "\t\tReduce shuffle bytes=7788690\n",
      "\t\tReduce input records=700000\n",
      "\t\tReduce output records=699921\n",
      "\t\tSpilled Records=1400000\n",
      "\t\tShuffled Maps =120\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=120\n",
      "\t\tGC time elapsed (ms)=3637\n",
      "\t\tCPU time spent (ms)=50520\n",
      "\t\tPhysical memory (bytes) snapshot=9651277824\n",
      "\t\tVirtual memory (bytes) snapshot=86718476288\n",
      "\t\tTotal committed heap usage (bytes)=8149008384\n",
      "\t\tPeak Map Physical memory (bytes)=295960576\n",
      "\t\tPeak Map Virtual memory (bytes)=2554044416\n",
      "\t\tPeak Reduce Physical memory (bytes)=233435136\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2573651968\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=39218102\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=37139296\n",
      "2025-04-15 21:49:50,505 INFO streaming.StreamJob: Output directory: /user/ubuntu/site_count_temp\n",
      "cat: Unable to write to output stream.\n",
      "cat: Unable to write to output stream.\n",
      "cat: Unable to write to output stream.\n",
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "hdfs dfs -ls /user/ubuntu/user_logs/\n",
    "\n",
    "# Clean previous output\n",
    "hdfs dfs -rm -r /user/ubuntu/site_count_temp 2> /dev/null || true\n",
    "\n",
    "# Run with verbose output\n",
    "hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "    -D mapreduce.job.name=\"site-count\" \\\n",
    "    -D mapreduce.job.reduces=4 \\\n",
    "    -files ./mapper1.py,./reducer1.py \\\n",
    "    -mapper \"python3 mapper1.py\" \\\n",
    "    -reducer \"python3 reducer1.py\" \\\n",
    "    -input /user/ubuntu/user_logs/user_logs.csv \\\n",
    "    -output /user/ubuntu/site_count_temp\n",
    "    \n",
    "hdfs dfs -ls /user/ubuntu/site_count_temp\n",
    "hdfs dfs -cat /user/ubuntu/site_count_temp/* | head -n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7da3543b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/ubuntu/top_sites_output\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-3.2.2.jar] /tmp/streamjob864865308747947462.jar tmpDir=null\n",
      "Found 2 items\n",
      "-rw-r--r--   1 ubuntu hadoop          0 2025-04-15 21:52 /user/ubuntu/top_sites_output/_SUCCESS\n",
      "-rw-r--r--   1 ubuntu hadoop   37090328 2025-04-15 21:52 /user/ubuntu/top_sites_output/part-00000\n",
      "2024-05-26 00:35:09.853479\thttp://www.knight.biz/\t1\n",
      "2024-05-26 00:35:10.853479\thttps://www.david-weaver.biz/\t1\n",
      "2024-05-26 00:35:10.853479\thttps://sullivan.com/\t1\n",
      "2024-05-26 00:35:10.853479\thttp://www.powers.com/\t1\n",
      "2024-05-26 00:35:12.853479\thttps://www.ortiz-hill.com/\t1\n",
      "2024-05-26 00:35:15.853479\thttps://chavez-cohen.net/\t1\n",
      "2024-05-26 00:35:15.853479\thttp://www.hanson.com/\t1\n",
      "2024-05-26 00:35:16.853479\thttps://www.schmitt.biz/\t1\n",
      "2024-05-26 00:35:16.853479\thttps://watkins-erickson.com/\t1\n",
      "2024-05-26 00:35:16.853479\thttp://mcgee.com/\t1\n",
      "2024-05-26 00:35:17.853479\thttps://brown.com/\t1\n",
      "2024-05-26 00:35:17.853479\thttp://www.butler-nguyen.com/\t1\n",
      "2024-05-26 00:35:19.853479\thttps://www.osborn.com/\t1\n",
      "2024-05-26 00:35:19.853479\thttp://nelson.com/\t1\n",
      "2024-05-26 00:35:20.853479\thttps://johnson-wright.com/\t1\n",
      "2024-05-26 00:35:20.853479\thttp://may-martinez.com/\t1\n",
      "2024-05-26 00:35:21.853479\thttp://stewart.com/\t1\n",
      "2024-05-26 00:35:23.853479\thttps://sims.net/\t1\n",
      "2024-05-26 00:35:23.853479\thttp://www.watson.com/\t1\n",
      "2024-05-26 00:35:24.853479\thttps://mercado.com/\t1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 21:52:04,629 INFO client.RMProxy: Connecting to ResourceManager at rc1b-dataproc-m-i45rlr66g11r9d3u.mdb.yandexcloud.net/10.129.0.25:8032\n",
      "2025-04-15 21:52:04,847 INFO client.AHSProxy: Connecting to Application History server at rc1b-dataproc-m-i45rlr66g11r9d3u.mdb.yandexcloud.net/10.129.0.25:10200\n",
      "2025-04-15 21:52:04,905 INFO client.RMProxy: Connecting to ResourceManager at rc1b-dataproc-m-i45rlr66g11r9d3u.mdb.yandexcloud.net/10.129.0.25:8032\n",
      "2025-04-15 21:52:04,914 INFO client.AHSProxy: Connecting to Application History server at rc1b-dataproc-m-i45rlr66g11r9d3u.mdb.yandexcloud.net/10.129.0.25:10200\n",
      "2025-04-15 21:52:05,177 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/ubuntu/.staging/job_1744740277109_0014\n",
      "2025-04-15 21:52:05,647 INFO mapred.FileInputFormat: Total input files to process : 4\n",
      "2025-04-15 21:52:05,758 INFO mapreduce.JobSubmitter: number of splits:32\n",
      "2025-04-15 21:52:05,947 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1744740277109_0014\n",
      "2025-04-15 21:52:05,949 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2025-04-15 21:52:06,151 INFO conf.Configuration: resource-types.xml not found\n",
      "2025-04-15 21:52:06,151 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2025-04-15 21:52:06,219 INFO impl.YarnClientImpl: Submitted application application_1744740277109_0014\n",
      "2025-04-15 21:52:06,252 INFO mapreduce.Job: The url to track the job: http://rc1b-dataproc-m-i45rlr66g11r9d3u.mdb.yandexcloud.net:8088/proxy/application_1744740277109_0014/\n",
      "2025-04-15 21:52:06,253 INFO mapreduce.Job: Running job: job_1744740277109_0014\n",
      "2025-04-15 21:52:12,465 INFO mapreduce.Job: Job job_1744740277109_0014 running in uber mode : false\n",
      "2025-04-15 21:52:12,466 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2025-04-15 21:52:20,591 INFO mapreduce.Job:  map 6% reduce 0%\n",
      "2025-04-15 21:52:21,604 INFO mapreduce.Job:  map 9% reduce 0%\n",
      "2025-04-15 21:52:22,615 INFO mapreduce.Job:  map 19% reduce 0%\n",
      "2025-04-15 21:52:27,688 INFO mapreduce.Job:  map 25% reduce 0%\n",
      "2025-04-15 21:52:31,732 INFO mapreduce.Job:  map 34% reduce 0%\n",
      "2025-04-15 21:52:32,740 INFO mapreduce.Job:  map 38% reduce 0%\n",
      "2025-04-15 21:52:33,749 INFO mapreduce.Job:  map 41% reduce 0%\n",
      "2025-04-15 21:52:34,758 INFO mapreduce.Job:  map 44% reduce 0%\n",
      "2025-04-15 21:52:38,794 INFO mapreduce.Job:  map 47% reduce 0%\n",
      "2025-04-15 21:52:39,811 INFO mapreduce.Job:  map 50% reduce 16%\n",
      "2025-04-15 21:52:40,829 INFO mapreduce.Job:  map 63% reduce 16%\n",
      "2025-04-15 21:52:44,866 INFO mapreduce.Job:  map 66% reduce 16%\n",
      "2025-04-15 21:52:45,871 INFO mapreduce.Job:  map 69% reduce 22%\n",
      "2025-04-15 21:52:48,894 INFO mapreduce.Job:  map 72% reduce 22%\n",
      "2025-04-15 21:52:49,903 INFO mapreduce.Job:  map 81% reduce 22%\n",
      "2025-04-15 21:52:50,909 INFO mapreduce.Job:  map 84% reduce 22%\n",
      "2025-04-15 21:52:51,917 INFO mapreduce.Job:  map 88% reduce 28%\n",
      "2025-04-15 21:52:54,939 INFO mapreduce.Job:  map 91% reduce 28%\n",
      "2025-04-15 21:52:55,947 INFO mapreduce.Job:  map 100% reduce 28%\n",
      "2025-04-15 21:52:57,962 INFO mapreduce.Job:  map 100% reduce 68%\n",
      "2025-04-15 21:53:00,985 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2025-04-15 21:53:02,008 INFO mapreduce.Job: Job job_1744740277109_0014 completed successfully\n",
      "2025-04-15 21:53:02,130 INFO mapreduce.Job: Counters: 56\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=7205683\n",
      "\t\tFILE: Number of bytes written=22774753\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=39172000\n",
      "\t\tHDFS: Number of bytes written=37090328\n",
      "\t\tHDFS: Number of read operations=101\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=3\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=32\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=25\n",
      "\t\tRack-local map tasks=7\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=189857\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=35276\n",
      "\t\tTotal time spent by all map tasks (ms)=189857\n",
      "\t\tTotal time spent by all reduce tasks (ms)=35276\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=189857\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=35276\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=194413568\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=36122624\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=699921\n",
      "\t\tMap output records=699921\n",
      "\t\tMap output bytes=37139296\n",
      "\t\tMap output materialized bytes=7561125\n",
      "\t\tInput split bytes=4800\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=414152\n",
      "\t\tReduce shuffle bytes=7561125\n",
      "\t\tReduce input records=699921\n",
      "\t\tReduce output records=698969\n",
      "\t\tSpilled Records=1399842\n",
      "\t\tShuffled Maps =32\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=32\n",
      "\t\tGC time elapsed (ms)=4755\n",
      "\t\tCPU time spent (ms)=42980\n",
      "\t\tPhysical memory (bytes) snapshot=9626132480\n",
      "\t\tVirtual memory (bytes) snapshot=84192694272\n",
      "\t\tTotal committed heap usage (bytes)=8057782272\n",
      "\t\tPeak Map Physical memory (bytes)=298905600\n",
      "\t\tPeak Map Virtual memory (bytes)=2558140416\n",
      "\t\tPeak Reduce Physical memory (bytes)=262561792\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2577571840\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=39167200\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=37090328\n",
      "2025-04-15 21:53:02,130 INFO streaming.StreamJob: Output directory: /user/ubuntu/top_sites_output\n",
      "bash: line 14: fg: no job control\n",
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Clean previous output\n",
    "hdfs dfs -rm -r /user/ubuntu/top_sites_output 2> /dev/null || true\n",
    "\n",
    "# Run with verbose output\n",
    "hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "    -D mapreduce.job.name=\"top-sites\" \\\n",
    "    -D mapreduce.job.reduces=1 \\\n",
    "    -files ./mapper2.py,./reducer2.py \\\n",
    "    -mapper \"python3 mapper2.py\" \\\n",
    "    -reducer \"python3 reducer2.py\" \\\n",
    "    -input /user/ubuntu/site_count_temp \\\n",
    "    -output /user/ubuntu/top_sites_output\n",
    "    \n",
    "hdfs dfs -ls /user/ubuntu/top_sites_output\n",
    "hdfs dfs -cat /user/ubuntu/top_sites_output/* | head -n 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088615b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
